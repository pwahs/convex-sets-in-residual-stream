{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9226890b",
   "metadata": {},
   "source": [
    "# Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29298c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import transformer_lens\n",
    "import gc\n",
    "\n",
    "#MODEL_NAME = \"EleutherAI/pythia-410m\"\n",
    "MODEL_NAME = \"EleutherAI/pythia-2.8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef4165a",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d081cc2",
   "metadata": {},
   "source": [
    "Read in names of basketballers and check their name is 2 tokens exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = Path(\"data\")\n",
    "# names_file = data_dir / \"names.txt\"\n",
    "# all_new_names = []\n",
    "\n",
    "# # Iterate over all csv files in the data directory\n",
    "# for csv_path in data_dir.glob(\"*.csv\"):\n",
    "#     # Read the csv file, assuming no header\n",
    "#     df = pd.read_csv(csv_path, header=None)\n",
    "#     # Get the second column (index 1) and drop any missing values\n",
    "#     names_from_csv = df.iloc[:, 1].dropna().tolist()\n",
    "#     filtered_names = [name for name in names_from_csv if isinstance(name, str) and name.count(' ') == 1]\n",
    "#     all_new_names.extend(filtered_names)\n",
    "\n",
    "# # Append the collected names to the names.txt file\n",
    "# if all_new_names:\n",
    "#     with open(names_file, \"a\") as f:\n",
    "#         # Ensure we start on a new line if the file isn't empty\n",
    "#         if names_file.stat().st_size > 0:\n",
    "#             f.write(\"\\n\")\n",
    "#         f.write(\"\\n\".join(all_new_names))\n",
    "#     print(f\"Appended {len(all_new_names)} names to {names_file}.\")\n",
    "# else:\n",
    "#     print(\"No new names found in CSV files to append.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeed21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_file = Path(\"data/names.txt\")\n",
    "with open(names_file, 'r') as f:\n",
    "    names = f.read().strip().split('\\n')\n",
    "    print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"model\" in locals() or \"model\" in globals():\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7022b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# 4. Verify memory is cleared\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# More detailed GPU memory info\n",
    "print(torch.cuda.memory_summary())\n",
    "\n",
    "# Load the Pythia-2.8b model\n",
    "# This will download the model weights if they are not already cached.\n",
    "model = transformer_lens.HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_NAME,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "    \n",
    "print(f\"{MODEL_NAME} model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "# 4. Verify memory is cleared\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# More detailed GPU memory info\n",
    "print(torch.cuda.memory_summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cfdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to store names that consist of exactly two tokens\n",
    "two_token_names = []\n",
    "\n",
    "for name in names:\n",
    "    # Tokenize the name. The result includes a batch dimension.\n",
    "    tokens = model.to_tokens(name)\n",
    "\n",
    "    # Check if the number of tokens (the length of the second dimension) is 3\n",
    "    # (<soe> token, first name, last name)\n",
    "    if tokens.shape[1] == 3:\n",
    "        two_token_names.append(name)\n",
    "    else:\n",
    "        print(f\"Name '{name}' is represented by {tokens}.\")\n",
    "\n",
    "print(f\"Found {len(two_token_names)} names that are exactly two tokens long.\")\n",
    "print(two_token_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225de2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basketball_players = []\n",
    "print(\"Completions for basketball players:\")\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "#two_token_names = player_names\n",
    "basketball_players = []\n",
    "\n",
    "for name in two_token_names:\n",
    "    prompt = f\"Fact: {name} is known for playing the sport of\"\n",
    "\n",
    "    # Generate a completion for the prompt\n",
    "    # We generate a few tokens and set temperature to 0 for deterministic output\n",
    "    completion = model.generate(\n",
    "        prompt, \n",
    "        max_new_tokens=2, \n",
    "        temperature=0,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Extract just the generated part of the text\n",
    "    completion_text = completion[len(prompt):].strip()\n",
    "\n",
    "    # Check if the completion is 'basketball' (case-insensitive)\n",
    "    if completion_text.lower().startswith(\"basketball\"):\n",
    "        basketball_players.append(name)\n",
    "    else:\n",
    "        print(completion)\n",
    "        print(completion_text)\n",
    "    print(f\"Prompt: '{prompt}' -> Completion: '{completion_text}'\")\n",
    "\n",
    "\n",
    "print(\"\\n---------------------------------------\")\n",
    "print(f\"Found {len(basketball_players)} names that completed with 'basketball':\")\n",
    "print(basketball_players)\n",
    "\n",
    "basketball_players_file = Path(\"data/basketball_players.txt\")\n",
    "with open(basketball_players_file, \"w\") as f:\n",
    "    f.write(\"\\n\".join(basketball_players))\n",
    "print(f\"Stored {len(basketball_players)} basketball players in {basketball_players_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(basketball_players)} basketball players identified and stored in {basketball_players_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42600dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "# Use the list of confirmed basketball players from the previous step\n",
    "# Extract first and last names\n",
    "first_names = list(set([name.split(' ')[0] for name in two_token_names]))\n",
    "last_names = list(set([name.split(' ')[1] for name in two_token_names]))\n",
    "\n",
    "# Create all possible name combinations\n",
    "all_combinations = [\" \".join(p) for p in itertools.product(first_names, last_names)]\n",
    "\n",
    "# Filter out the names that actually exist\n",
    "real_names_set = set(two_token_names)\n",
    "fake_names = [name for name in all_combinations if name not in real_names_set]\n",
    "# Pick 100 random fake names\n",
    "fake_names = random.sample(fake_names, min(100, len(fake_names)))\n",
    "\n",
    "print(f\"Generated {len(fake_names)} fake names to test.\")\n",
    "\n",
    "non_basketball_fake_players = []\n",
    "non_basketball_completions = []\n",
    "\n",
    "# Run basketball detection on the fake names\n",
    "for i, name in enumerate(fake_names):\n",
    "    prompt = f\"Fact: {name} is known for playing the sport of\"\n",
    "\n",
    "    completion = model.generate(\n",
    "        prompt, \n",
    "        max_new_tokens=2, \n",
    "        temperature=0,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    completion_text = completion[len(prompt):].strip()\n",
    "\n",
    "    # Keep the names that do NOT complete with \"basketball\"\n",
    "    if not completion_text.lower().startswith(\"basketball\"):\n",
    "        non_basketball_fake_players.append(name)\n",
    "        non_basketball_completions.append(completion)\n",
    "    if i%10 == 9:\n",
    "        print(f\"Processed {i+1} fake names... Ex: {completion}\")\n",
    "\n",
    "print(f\"\\nFound {len(non_basketball_fake_players)} fake names that are not associated with basketball.\")\n",
    "print(\"A sample of non-basketball completions for fake names:\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "# Define the sample size\n",
    "sample_size = min(20, len(non_basketball_completions))\n",
    "\n",
    "# Print a random sample of the results\n",
    "if sample_size > 0:\n",
    "    for i in random.sample(range(len(non_basketball_completions)), sample_size):\n",
    "        print(non_basketball_completions[i])\n",
    "\n",
    "fake_basketball_players_file = Path(\"data/fake_basketball_players.txt\")\n",
    "with open(fake_basketball_players_file, \"w\") as f:\n",
    "    f.write(\"\\n\".join(non_basketball_fake_players))\n",
    "print(f\"Stored {len(non_basketball_fake_players)} fake basketball players in {fake_basketball_players_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd8b5c",
   "metadata": {},
   "source": [
    "# Compute and cache Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer_lens\n",
    "import gc\n",
    "\n",
    "# Clear memory\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load model with memory optimizations\n",
    "model = transformer_lens.HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_NAME, device=\"cuda\", dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Function to process one name with memory optimization\n",
    "def process_with_cache(name):\n",
    "    prompt = f\"Fact: {name}\"\n",
    "    \n",
    "    # Temporary GPU usage with tight memory control\n",
    "    # with torch.inference_mode():\n",
    "        # model.to(\"cuda\")\n",
    "        \n",
    "        # Key optimizations in run_with_cache:\n",
    "        # - remove_batch_dim=True reduces tensor dimensions\n",
    "        # - names_filter lets you select specific layers to cache\n",
    "        # - incl_bwd=False avoids caching backward hooks\n",
    "    _, cache = model.run_with_cache(\n",
    "        prompt, \n",
    "        remove_batch_dim=True,\n",
    "        names_filter=None,  # Set to specific layers if needed\n",
    "        incl_bwd=False\n",
    "    )\n",
    "        \n",
    "        # # Immediately move cache to CPU\n",
    "        # for k, v in cache.items():\n",
    "        #     cache[k] = v.detach().cpu()\n",
    "        \n",
    "        # Move model back to CPU and clear GPU\n",
    "        # model.to(\"cpu\")\n",
    "        # torch.cuda.empty_cache()\n",
    "    \n",
    "    return cache\n",
    "\n",
    "def compute_caches(names):\n",
    "    caches = {}\n",
    "    with torch.inference_mode():\n",
    "        # Process each name individually and save to disk\n",
    "        for i, name in enumerate(names):\n",
    "            try:\n",
    "                caches[name] = process_with_cache(name)\n",
    "                # Save each cache separately to avoid accumulating in memory\n",
    "                # torch.save(cache, f\"cache_{i}_{name.replace(' ', '_')}.pt\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {name}: {e}\")\n",
    "\n",
    "            # Force cleanup\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    return caches\n",
    "\n",
    "# Load player names\n",
    "with open(\"data/basketball_players.txt\", \"r\") as f:\n",
    "    player_names = [line.strip() for line in f if line.strip()]\n",
    "with open(\"data/fake_basketball_players.txt\", \"r\") as f:\n",
    "    fake_names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "player_caches = compute_caches(player_names)\n",
    "print(f\"Processed {len(player_caches)} player caches.\")\n",
    "fake_caches = compute_caches(fake_names)\n",
    "print(f\"Processed {len(fake_caches)} fake player caches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d29849",
   "metadata": {},
   "source": [
    "# Compute convex covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69626c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_point_thick(vector, extra_vectors = 100, thickness=0.2):\n",
    "    # Create the points +- thickness in each dimension, return vector of all points and original\n",
    "    vectors = [vector]\n",
    "    for dim in range(min(extra_vectors, len(vector))):\n",
    "        # Create a new vector with the current dimension increased and decreased by thickness\n",
    "        pos_vector = vector.copy()\n",
    "        pos_vector[dim] += thickness\n",
    "        vectors.append(pos_vector)\n",
    "    return vectors\n",
    "\n",
    "def get_point_set(caches, hook_name = \"blocks.0.hook_resid_post\", extra_vectors = 100, thickness = 0.2):\n",
    "    # First, check the shape of one vector to initialize properly\n",
    "    sample_name = list(caches.keys())[0]\n",
    "    sample_vector = caches[sample_name][hook_name][-1]\n",
    "    vector_dim = sample_vector.shape[0]\n",
    "    print(f\"Vector dimension: {vector_dim}\")\n",
    "\n",
    "    # Create a list to hold all vectors\n",
    "    vectors = []\n",
    "\n",
    "    # Extract vectors from all players\n",
    "    print(\"Collecting vectors from cache...\")\n",
    "    for name in caches.keys():\n",
    "        try:\n",
    "            # Extract the vector for this player at the last position [-1]\n",
    "            vector = caches[name][hook_name][-1]\n",
    "            \n",
    "            # Make sure it's on CPU and detached (in case it isn't already)\n",
    "            vector = vector.detach().cpu()\n",
    "            # Convert the tensor to a numpy array\n",
    "            vector = vector.numpy()\n",
    "            # Add to our collection\n",
    "            vectors.extend(make_point_thick(vector, extra_vectors = extra_vectors, thickness = thickness))\n",
    "        except KeyError as e:\n",
    "            print(f\"Skipping {name}: Missing '{hook_name}' in cache\")\n",
    "        except IndexError as e:\n",
    "            print(f\"Skipping {name}: No last element in '{hook_name}'\")\n",
    "    return vectors\n",
    "player_post_layer = get_point_set(player_caches, \"blocks.0.hook_resid_post\", extra_vectors=0)\n",
    "fake_post_layer = get_point_set(fake_caches, \"blocks.0.hook_resid_post\", extra_vectors=0)\n",
    "print(f\"{len(player_post_layer)} player vectors collected, {len(fake_post_layer)} fake vectors collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distance_stats(set_a, set_b, stats_lists=None, equal = False):\n",
    "    \"\"\"\n",
    "    Calculate and print distance statistics between two sets of vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - set_a: First set of vectors\n",
    "    - set_b: Second set of vectors\n",
    "    - stats_lists: Optional dictionary containing lists to append stats to\n",
    "                  with keys 'min', 'max', 'mean', 'median'\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for i, a in enumerate(set_a):\n",
    "        for j, b in enumerate(set_b):\n",
    "            distance = np.linalg.norm(a - b)\n",
    "            if equal and i == j: \n",
    "                # Skip distances between the same points\n",
    "                continue\n",
    "            distances.append(distance)\n",
    "    distances.sort()\n",
    "    print(\n",
    "        f\"Points in A: {len(set_a)}, in B: {len(set_b)}, total distances: {len(distances)}\"\n",
    "    )\n",
    "    print(f\"Min: {distances[0]:.4f}, Mean: {np.mean(distances):.4f}, Median: {np.median(distances):.4f}, Max: {distances[-1]:.4f}\")\n",
    "    \n",
    "    if stats_lists is not None:\n",
    "        if 'min' in stats_lists:\n",
    "            stats_lists['min'].append(distances[0])\n",
    "        if 'max' in stats_lists:\n",
    "            stats_lists['max'].append(distances[-1])\n",
    "        if 'mean' in stats_lists:\n",
    "            stats_lists['mean'].append(np.mean(distances))\n",
    "        if 'median' in stats_lists:\n",
    "            stats_lists['median'].append(np.median(distances))\n",
    "\n",
    "\n",
    "print_distance_stats(player_post_layer, fake_post_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c754bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "# Combine all vectors (players and fake)\n",
    "all_vectors = np.vstack([player_post_layer, fake_post_layer])\n",
    "labels = [\"Player\"] * len(player_post_layer) + [\"Fake\"] * len(fake_post_layer)\n",
    "\n",
    "# Method 1: t-SNE visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_tsne = tsne.fit_transform(all_vectors)\n",
    "\n",
    "# # Method 2: UMAP (often better preserves global structure)\n",
    "# reducer = umap.UMAP(random_state=42)\n",
    "# embeddings_umap = reducer.fit_transform(all_vectors)\n",
    "\n",
    "# Create plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot t-SNE\n",
    "sns.scatterplot(x=embeddings_tsne[:, 0], y=embeddings_tsne[:, 1], \n",
    "                hue=labels, ax=ax1, palette=\"Set1\")\n",
    "ax1.set_title(\"t-SNE Visualization\")\n",
    "\n",
    "# Plot UMAP\n",
    "# sns.scatterplot(x=embeddings_umap[:, 0], y=embeddings_umap[:, 1], \n",
    "                # hue=labels, ax=ax2, palette=\"Set1\")\n",
    "# ax2.set_title(\"UMAP Visualization\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e33ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate within-group and between-group distances\n",
    "player_player_distances = []\n",
    "fake_fake_distances = []\n",
    "player_fake_distances = []\n",
    "\n",
    "for i in range(len(all_vectors)):\n",
    "    for j in range(i+1, len(all_vectors)):\n",
    "        dist = np.linalg.norm(all_vectors[i] - all_vectors[j])\n",
    "        if i < len(player_post_layer) and j < len(player_post_layer):\n",
    "            player_player_distances.append(dist)\n",
    "        elif i >= len(player_post_layer) and j >= len(player_post_layer):\n",
    "            fake_fake_distances.append(dist)\n",
    "        else:\n",
    "            player_fake_distances.append(dist)\n",
    "\n",
    "# Plot distribution of distances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(player_player_distances, label=\"Player-Player\", fill=True)\n",
    "sns.kdeplot(fake_fake_distances, label=\"Fake-Fake\", fill=True)\n",
    "sns.kdeplot(player_fake_distances, label=\"Player-Fake\", fill=True)\n",
    "plt.xlabel(\"Euclidean Distance\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of Distances Between Vector Groups\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convex_point_cover.algorithms.kruskal import kruskal\n",
    "\n",
    "num_layers = model.W_K.shape[0]\n",
    "\n",
    "layers = range(num_layers)\n",
    "stats = {'min': [], 'max': [], 'mean': [], 'median': []}\n",
    "stats_player_to_0 = {'min': [], 'max': [], 'mean': [], 'median': []}\n",
    "stats_fake_to_0 = {'min': [], 'max': [], 'mean': [], 'median': []}\n",
    "\n",
    "cluster_sizes = [[], []]\n",
    "for layer in range(0, num_layers, 1):\n",
    "    player_post_layer = get_point_set(player_caches, f\"blocks.{layer}.hook_resid_post\", extra_vectors = 0, thickness = 0.2)\n",
    "    fake_post_layer = get_point_set(fake_caches, f\"blocks.{layer}.hook_resid_post\", extra_vectors = 0, thickness = 0.2)\n",
    "    print(\n",
    "        f\"Layer {layer}: {len(player_post_layer)} player vectors collected, {len(fake_post_layer)} fake vectors collected.\"\n",
    "    )\n",
    "\n",
    "    print(\"Distances between player vectors and fake vectors:\")\n",
    "    print_distance_stats(player_post_layer, fake_post_layer, stats_lists=stats)\n",
    "    zero_vector = np.zeros_like(player_post_layer[0])\n",
    "    print(\"Distances to 0:\")\n",
    "    print_distance_stats(\n",
    "        player_post_layer,\n",
    "        [zero_vector],\n",
    "        stats_lists=stats_player_to_0,\n",
    "    )\n",
    "    print(\"fakes to 0:\")\n",
    "    print_distance_stats(\n",
    "        fake_post_layer,\n",
    "        [zero_vector],\n",
    "        stats_lists=stats_fake_to_0,\n",
    "    )\n",
    "\n",
    "# Plot the evolution of distances across layers\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(layers, stats['min'], marker=\"o\", label=\"Min Distance\")\n",
    "plt.plot(layers, stats['median'], marker=\"s\", label=\"Median Distance\")\n",
    "plt.plot(layers, stats['mean'], marker=\"^\", label=\"Mean Distance\")\n",
    "plt.plot(layers, stats['max'], marker=\"*\", label=\"Max Distance\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Distance between points\")\n",
    "plt.title(\"Distance Metrics Across Layers\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(layers, stats_player_to_0[\"min\"], marker=\"o\", label=\"Min Distance\")\n",
    "plt.plot(layers, stats_player_to_0[\"median\"], marker=\"s\", label=\"Median Distance\")\n",
    "plt.plot(layers, stats_player_to_0[\"mean\"], marker=\"^\", label=\"Mean Distance\")\n",
    "plt.plot(layers, stats_player_to_0[\"max\"], marker=\"*\", label=\"Max Distance\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Distance from 0\")\n",
    "plt.title(\"Magnitude Metrics Across Layers for real names\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(layers, stats_fake_to_0[\"min\"], marker=\"o\", label=\"Min Distance\")\n",
    "plt.plot(layers, stats_fake_to_0[\"median\"], marker=\"s\", label=\"Median Distance\")\n",
    "plt.plot(layers, stats_fake_to_0[\"mean\"], marker=\"^\", label=\"Mean Distance\")\n",
    "plt.plot(layers, stats_fake_to_0[\"max\"], marker=\"*\", label=\"Max Distance\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Distance from 0\")\n",
    "plt.title(\"Magnitude Metrics Across Layers for fake names\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72697fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how meaningfully different prefixed tokenizations are\n",
    "name = \"Michael Jordan\"\n",
    "\n",
    "for i in range(50):\n",
    "    prefix = \" \" * i\n",
    "    prompt = f\"Fact: {prefix}{name}\"\n",
    "\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    print(f\"Tokens for '{prompt}': {tokens}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how meaningfully different prefixed embeddings are\n",
    "name = \"Michael Jordan\"\n",
    "prefixed_names = [\" \" * i + name for i in range(50)]\n",
    "cache = compute_caches(prefixed_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896cafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stats = {\"min\": [], \"max\": [], \"mean\": [], \"median\": []}\n",
    "\n",
    "for layer in range(model.W_V.shape[0]):\n",
    "    vectors = []\n",
    "    for names in prefixed_names:\n",
    "        vectors.append(cache[names][f\"blocks.{layer}.hook_resid_post\"].detach().cpu().numpy()[-1])\n",
    "    print_distance_stats(vectors, vectors, stats_lists=stats, equal = True)\n",
    "\n",
    "# Plot the evolution of distances across layers\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(layers, stats[\"min\"], marker=\"o\", label=\"Min Distance\")\n",
    "plt.plot(layers, stats[\"median\"], marker=\"s\", label=\"Median Distance\")\n",
    "plt.plot(layers, stats[\"mean\"], marker=\"^\", label=\"Mean Distance\")\n",
    "plt.plot(layers, stats[\"max\"], marker=\"*\", label=\"Max Distance\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Distance between points\")\n",
    "plt.title(\"Distance of space-prefixed Embeddings Across Layers\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d743e4",
   "metadata": {},
   "source": [
    "# Increase data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store results\n",
    "results_per_prefix = []\n",
    "results_per_name = {name.strip():0 for name in player_names}\n",
    "\n",
    "# Loop through different numbers of spaces\n",
    "for num_spaces in range(0, 1000, 10):\n",
    "    prefix = \" \" * num_spaces\n",
    "    basketball_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    # Test each player name\n",
    "    for name in results_per_name.keys():\n",
    "        prompt = f\"Fact: {prefix}{name} is known for playing the sport of\"\n",
    "        completion = model.generate(\n",
    "            prompt, \n",
    "            max_new_tokens=2, \n",
    "            temperature=0,\n",
    "            verbose=False\n",
    "        )\n",
    "        completion_text = completion[len(prompt):].strip()\n",
    "        \n",
    "        # Count basketball completions\n",
    "        if completion_text.lower().startswith(\"basketball\"):\n",
    "            basketball_count += 1\n",
    "            results_per_name[name] += 1\n",
    "        total_count += 1\n",
    "        \n",
    "        # Print only a sample of the results to avoid flooding output\n",
    "        if len(results_per_prefix) < 3 or num_spaces % 10 == 0:\n",
    "            print(f\"Prompt: '{prompt}' -> Completion: '{completion_text}'\")\n",
    "    \n",
    "    # Store the results for this number of spaces\n",
    "    percentage = (basketball_count / total_count) * 100 if total_count > 0 else 0\n",
    "    results_per_prefix.append((num_spaces, basketball_count, total_count, percentage))\n",
    "    print(f\"Spaces: {num_spaces}, Basketball: {basketball_count}/{total_count} ({percentage:.2f}%)\")\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spaces = [r[0] for r in results_per_prefix]\n",
    "percentages = [r[3] for r in results_per_prefix]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(spaces, percentages, marker='o')\n",
    "plt.title('Percentage of \"basketball\" completions vs. number of spaces')\n",
    "plt.xlabel('Number of spaces')\n",
    "plt.ylabel('Percentage of basketball completions')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Sort players by their count of basketball completions\n",
    "sorted_players = sorted(results_per_name.items(), key=lambda x: x[1], reverse=True)\n",
    "player_names_sorted = [player[0] for player in sorted_players]\n",
    "completion_counts = [player[1] for player in sorted_players]\n",
    "\n",
    "# Create a horizontal bar chart for better readability with many player names\n",
    "plt.figure(figsize=(10, 12))\n",
    "bars = plt.barh(player_names_sorted, completion_counts)\n",
    "\n",
    "# Add count labels to the bars\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_width() + 0.2, bar.get_y() + bar.get_height()/2, \n",
    "             str(completion_counts[i]), \n",
    "             va='center')\n",
    "\n",
    "plt.xlabel('Number of \"basketball\" completions')\n",
    "plt.ylabel('Player Name')\n",
    "plt.title('Sensitivity of Player Names to Spacing (steps of 10)\\n(Higher = More Robust Basketball Association)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"Average basketball completions per player: {sum(completion_counts)/len(completion_counts):.2f}\")\n",
    "print(f\"Max basketball completions: {max(completion_counts)}\")\n",
    "print(f\"Min basketball completions: {min(completion_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convex_point_cover.algorithms.fast_kruskal import fast_kruskal\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "num_layers = model.W_K.shape[0]\n",
    "layers = range(num_layers)\n",
    "\n",
    "def process_layer(layer, player_vectors, fake_vectors):\n",
    "    positive_clusters = fast_kruskal(\n",
    "        player_vectors,\n",
    "        fake_vectors,\n",
    "        epsilon=0.1,\n",
    "        debug=False,\n",
    "    )\n",
    "    negative_clusters = fast_kruskal(\n",
    "        fake_vectors,\n",
    "        player_vectors,\n",
    "        epsilon=0.1,\n",
    "        debug=False,\n",
    "    )\n",
    "    return {\n",
    "        \"layer\": layer,\n",
    "        \"player_count\": len(player_vectors),\n",
    "        \"fake_count\": len(fake_vectors),\n",
    "        \"positive_clusters\": len(positive_clusters),\n",
    "        \"negative_clusters\": len(negative_clusters),\n",
    "    }\n",
    "\n",
    "\n",
    "# Run parallel computation - will use all available cores\n",
    "print(\"Processing layers in parallel...\")\n",
    "results = Parallel(n_jobs=-1, verbose=10)(\n",
    "    delayed(process_layer)(\n",
    "        layer,\n",
    "        get_point_set(\n",
    "            player_caches,\n",
    "            f\"blocks.{layer}.hook_resid_post\",\n",
    "            extra_vectors=0,\n",
    "            thickness=0.2,\n",
    "        ),\n",
    "        get_point_set(\n",
    "            fake_caches,\n",
    "            f\"blocks.{layer}.hook_resid_post\",\n",
    "            extra_vectors=0,\n",
    "            thickness=0.2,\n",
    "        ),\n",
    "    )\n",
    "    for layer in layers\n",
    ")\n",
    "\n",
    "# Process and display results\n",
    "cluster_sizes = [[], []]\n",
    "for result in sorted(results, key=lambda x: x[\"layer\"]):\n",
    "    layer = result[\"layer\"]\n",
    "    player_count = result[\"player_count\"]\n",
    "    fake_count = result[\"fake_count\"]\n",
    "    pos_clusters = result[\"positive_clusters\"]\n",
    "    neg_clusters = result[\"negative_clusters\"]\n",
    "\n",
    "    # Store results\n",
    "    cluster_sizes[0].append(pos_clusters)\n",
    "    cluster_sizes[1].append(neg_clusters)\n",
    "\n",
    "    # Print layer info\n",
    "    print(\n",
    "        f\"Layer {layer}: {player_count} player vectors collected, {fake_count} fake vectors collected.\"\n",
    "    )\n",
    "    print(f\"{pos_clusters} clusters found.\")\n",
    "    print(f\"{neg_clusters} negative clusters found.\")\n",
    "\n",
    "# Plot the number of clusters found across layers\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(layers, cluster_sizes[0], marker=\"o\", label=\"Positive Clusters\")\n",
    "plt.plot(layers, cluster_sizes[1], marker=\"s\", label=\"Negative Clusters\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Number of Clusters\")\n",
    "plt.title(\"Number of Clusters Found Across Layers\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebbf85b",
   "metadata": {},
   "source": [
    "# Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0229dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def count_linear_separation(set_a, set_b):\n",
    "    # Combine all vectors into a dataset and create labels\n",
    "    X = np.vstack([set_a, set_b])\n",
    "    y = np.array([\"Player\"] * len(set_a) + [\"Fake\"] * len(set_b))\n",
    "\n",
    "    # Train a linear SVM classifier\n",
    "    clf = LinearSVC(random_state=42, max_iter=10000)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X)\n",
    "\n",
    "    # Count overall misclassifications\n",
    "    misclassified = np.sum(y_pred != y)\n",
    "    print(f\"Total misclassified points: {misclassified} out of {len(y)} ({misclassified/len(y)*100:.2f}%)\")\n",
    "\n",
    "    # Get misclassified by class\n",
    "    fake_misclassified = np.sum((y_pred != y) & (y_pred == \"Player\") & (y == \"Fake\"))\n",
    "    player_misclassified = np.sum((y_pred != y) & (y_pred == \"Fake\") & (y == \"Player\"))\n",
    "\n",
    "    print(f\"Fake points misclassified as players: {fake_misclassified}\")\n",
    "    print(f\"Player points misclassified as fake: {player_misclassified}\")\n",
    "    return fake_misclassified, player_misclassified\n",
    "\n",
    "print(count_linear_separation(player_post_layer, fake_post_layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b1d4c",
   "metadata": {},
   "source": [
    "# Putting the pieces together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d63a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pweiss/Research/mats/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import transformer_lens\n",
    "import gc\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# MODEL_NAME = \"EleutherAI/pythia-410m\"\n",
    "MODEL_NAME = \"EleutherAI/pythia-2.8b\"\n",
    "\n",
    "# Data generation\n",
    "READ_FROM_FILE = True # If false, re-check all basketballer predictions\n",
    "DATA_FOLDER = \"data-2.8b\"\n",
    "SPACE_RANGE = range(7, 5000, 7) # How many different spaces real data points get\n",
    "FAKE_NAMES_MAX = 2000 # how many different fake names to generate\n",
    "NUM_PREFIXES = 100 # How many different spaces fake data points get\n",
    "LIMIT = 20000 # Max number of total data points\n",
    "\n",
    "MAX_BATCH_SIZE = 1000\n",
    "\n",
    "NUM_JOBS = -1 # How many parallel jobs for computing convex sets (max 1 per layer, -1 for all cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4fa78f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.00 MB\n",
      "GPU memory reserved: 0.00 MB\n",
      "Loaded pretrained model EleutherAI/pythia-2.8b into HookedTransformer\n",
      "EleutherAI/pythia-2.8b model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "if \"model\" in locals() or \"model\" in globals():\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# 4. Verify memory is cleared\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Load the Pythia-2.8b model\n",
    "# This will download the model weights if they are not already cached.\n",
    "model = transformer_lens.HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_NAME,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "\n",
    "print(f\"{MODEL_NAME} model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "405a62a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3962 names\n"
     ]
    }
   ],
   "source": [
    "names_file = Path(f\"{DATA_FOLDER}/names.txt\")\n",
    "with open(names_file, \"r\") as f:\n",
    "    names = f.read().strip().split(\"\\n\")\n",
    "    print(f\"{len(names)} names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ef8c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 372 names that are exactly two tokens long.\n",
      "['Alex English', 'Gary Neal', 'James Johnson', 'Gordon Herbert', 'Robert Parish', 'Charles Smith', 'Rod Strickland', 'Kevin Edwards', 'Scott Brooks', 'Tony Gonzalez']\n"
     ]
    }
   ],
   "source": [
    "# A list to store names that consist of exactly two tokens\n",
    "two_token_names = []\n",
    "\n",
    "for name in names:\n",
    "    # Tokenize the name. The result includes a batch dimension.\n",
    "    tokens = model.to_tokens(name)\n",
    "\n",
    "    # Check if the number of tokens (the length of the second dimension) is 3\n",
    "    # (<soe> token, first name, last name)\n",
    "    if tokens.shape[1] == 3:\n",
    "        two_token_names.append(name)\n",
    "\n",
    "print(f\"Found {len(two_token_names)} names that are exactly two tokens long.\")\n",
    "print(two_token_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46a0626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Michael Jordan is known for playing the sport of basketball. He is also\n",
      "['Fact: Michael Jordan is known for playing the sport of basketball', 'Fact:  Michael Jordan is known for playing the sport of basketball']\n"
     ]
    }
   ],
   "source": [
    "# Make absolutely sure the strings are identical\n",
    "text = \"Fact: Michael Jordan is known for playing the sport of\"\n",
    "\n",
    "# Single string\n",
    "result1 = model.generate(text, max_new_tokens=5, temperature=0, verbose=False)\n",
    "print(result1)\n",
    "\n",
    "# Batch with identical strings\n",
    "result2 = model.generate(\n",
    "    [\n",
    "        text,\n",
    "        \"Fact:  Michael Jordan is known for playing the sport of\",\n",
    "    ],  # Use the exact same string variable\n",
    "    max_new_tokens=1,\n",
    "    temperature=0,\n",
    "    verbose=False,\n",
    "    padding_side='left'\n",
    ")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16fde73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41939 basketball players from data-2.8b/basketball_players.txt\n"
     ]
    }
   ],
   "source": [
    "def is_basketball_player(name):\n",
    "    prompt = f\"Fact: {name} is known for playing the sport of\"\n",
    "    completion = model.generate(prompt, max_new_tokens=1, temperature=0, verbose=False)\n",
    "    completion_text = completion[len(prompt) :].strip()\n",
    "    return completion_text.lower().startswith(\"basketball\")\n",
    "\n",
    "prefix_candidates = [\" \" * i for i in SPACE_RANGE]\n",
    "\n",
    "if READ_FROM_FILE:\n",
    "    # Read basketball players from file\n",
    "    basketball_players_file = Path(f\"{DATA_FOLDER}/basketball_players.txt\")\n",
    "    with open(basketball_players_file, \"r\") as f:\n",
    "        basketball_players = [line.rstrip() for line in f if line.rstrip()]\n",
    "    print(f\"Loaded {len(basketball_players)} basketball players from {basketball_players_file}\")\n",
    "else:\n",
    "    # basketball_players = []\n",
    "    print(\"Completions for basketball players:\")\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    # two_token_names = player_names\n",
    "    basketball_players = []\n",
    "\n",
    "    for name in two_token_names:\n",
    "        if not is_basketball_player(name):\n",
    "            continue\n",
    "        basketball_players.append(name)\n",
    "        print(\".\", end=\"\")\n",
    "        # Player without prefix passed, try to generate more data points:\n",
    "        prefixed_names = [prefix + name for prefix in prefix_candidates]\n",
    "        attempted = 0\n",
    "        found = 0\n",
    "        for prefixed_name in prefixed_names:\n",
    "            attempted += 1\n",
    "            if is_basketball_player(prefixed_name):\n",
    "                basketball_players.append(prefixed_name)\n",
    "                found += 1\n",
    "            if attempted == 10 and found == 0:\n",
    "                # If I can't find a second data point in the first 10 tries, abort\n",
    "                break\n",
    "\n",
    "    print(\"\\n---------------------------------------\")\n",
    "    print(f\"Found {len(basketball_players)} names that completed with 'basketball':\")\n",
    "    print(basketball_players[:10])\n",
    "\n",
    "    random.shuffle(basketball_players)\n",
    "\n",
    "    basketball_players_file = Path(f\"{DATA_FOLDER}/basketball_players.txt\")\n",
    "    with open(basketball_players_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(basketball_players))\n",
    "    print(\n",
    "        f\"Stored {len(basketball_players)} basketball players in {basketball_players_file}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08c6330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 129893 basketball players from data-2.8b/fake_basketball_players.txt\n"
     ]
    }
   ],
   "source": [
    "# READ_FROM_FILE = False\n",
    "\n",
    "if READ_FROM_FILE:\n",
    "    fake_basketball_players_file = Path(f\"{DATA_FOLDER}/fake_basketball_players.txt\")\n",
    "    with open(fake_basketball_players_file, \"r\") as f:\n",
    "        non_basketball_players = [line.rstrip() for line in f if line.rstrip()]\n",
    "    print(\n",
    "        f\"Loaded {len(non_basketball_players)} basketball players from {fake_basketball_players_file}\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # Use the list of confirmed basketball players from the previous step\n",
    "    # Extract first and last names\n",
    "    first_names = list(set([name.split(\" \")[0] for name in basketball_players]))\n",
    "    last_names = list(set([name.split(\" \")[1] for name in basketball_players]))\n",
    "\n",
    "    # Create all possible name combinations\n",
    "    all_combinations = [\" \".join(p) for p in itertools.product(first_names, last_names)]\n",
    "\n",
    "    # Filter out the names that actually exist\n",
    "    real_names_set = set(basketball_players)\n",
    "    fake_names = [name for name in all_combinations if name not in real_names_set]\n",
    "    # Pick 100 random fake names\n",
    "    fake_names = random.sample(fake_names, min(FAKE_NAMES_MAX, len(fake_names)))\n",
    "    # Add random prefixes:\n",
    "    prefixed_fake_names = [prefix + name for name in fake_names for prefix in random.sample(prefix_candidates, NUM_PREFIXES)]\n",
    "\n",
    "    # Combine original fake names with prefixed ones\n",
    "    fake_names = fake_names + prefixed_fake_names\n",
    "\n",
    "    print(f\"Generated {len(fake_names)} fake names to test.\")\n",
    "    print(f\"{fake_names[-20:]}\")\n",
    "\n",
    "    non_basketball_fake_players = []\n",
    "    non_basketball_completions = []\n",
    "\n",
    "    # Run basketball detection on the fake names\n",
    "    for i, name in enumerate(fake_names):\n",
    "        if not is_basketball_player(name):\n",
    "            non_basketball_fake_players.append(name)\n",
    "        if i % 100 == 9:\n",
    "            print(\".\", end=\"\")\n",
    "\n",
    "    random.shuffle(non_basketball_fake_players)\n",
    "\n",
    "    print(\n",
    "        f\"\\nFound {len(non_basketball_fake_players)} fake names that are not associated with basketball.\"\n",
    "    )\n",
    "\n",
    "    fake_basketball_players_file = Path(f\"{DATA_FOLDER}/fake_basketball_players.txt\")\n",
    "    with open(fake_basketball_players_file, \"w\") as f:\n",
    "        f.write(\"\\n\".join(non_basketball_fake_players))\n",
    "    print(\n",
    "        f\"Stored {len(non_basketball_fake_players)} fake basketball players in {fake_basketball_players_file}.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dae3897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af0366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6602, -0.3262,  0.0781,  ..., -0.3164, -0.6797,  1.2266],\n",
      "        [ 0.4297, -0.0430, -0.3379,  ..., -0.9883, -0.7070, -0.1836],\n",
      "        [ 0.1992,  0.8477,  0.1543,  ...,  0.6016,  0.2324, -0.2295],\n",
      "        [-0.2891,  0.1484, -0.1592,  ..., -0.0361,  0.1963,  0.0898],\n",
      "        [-0.0352,  0.3164, -0.1152,  ...,  0.3066,  0.3203,  0.3340],\n",
      "        [ 0.2930, -0.0649,  0.0771,  ..., -0.1904,  0.7891,  0.5625]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "blocks.0.hook_resid_pre: torch.Size([6, 2560])\n",
      "blocks.0.ln1.hook_scale: torch.Size([6, 1])\n",
      "blocks.0.ln1.hook_normalized: torch.Size([6, 2560])\n",
      "blocks.0.attn.hook_q: torch.Size([6, 32, 80])\n",
      "blocks.0.attn.hook_k: torch.Size([6, 32, 80])\n",
      "blocks.0.attn.hook_v: torch.Size([6, 32, 80])\n",
      "blocks.0.attn.hook_rot_q: torch.Size([6, 32, 80])\n",
      "blocks.0.attn.hook_rot_k: torch.Size([6, 32, 80])\n",
      "blocks.0.attn.hook_attn_scores: torch.Size([32, 6, 6])\n",
      "blocks.0.attn.hook_pattern: torch.Size([32, 6, 6])\n",
      "blocks.0.attn.hook_z: torch.Size([6, 32, 80])\n",
      "blocks.0.hook_attn_out: torch.Size([6, 2560])\n",
      "blocks.0.ln2.hook_scale: torch.Size([6, 1])\n",
      "blocks.0.ln2.hook_normalized: torch.Size([6, 2560])\n",
      "blocks.0.mlp.hook_pre: torch.Size([6, 10240])\n",
      "blocks.0.mlp.hook_post: torch.Size([6, 10240])\n",
      "blocks.0.hook_mlp_out: torch.Size([6, 2560])\n",
      "blocks.0.hook_resid_post: torch.Size([6, 2560])\n",
      "blocks.0.hook_resid_pre: tensor([ 0.0079,  0.0133, -0.0061,  ...,  0.0050, -0.0320,  0.0205],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "blocks.0.hook_attn_out: tensor([-0.3359,  0.0762, -0.1445,  ..., -0.1572, -0.0400,  0.2891],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "blocks.0.hook_mlp_out: tensor([ 0.6211, -0.1543,  0.2275,  ..., -0.0378,  0.8633,  0.2539],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "blocks.0.hook_resid_post: tensor([ 0.2930, -0.0649,  0.0771,  ..., -0.1904,  0.7891,  0.5625],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([ 0.2930, -0.0649,  0.0771,  ..., -0.1904,  0.7891,  0.5625],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# Switching to batched processing. Different token lengths mess up the values via padding, process\n",
    "# same lengths together, this checks that the results are the same.\n",
    "input = [\"Prompt: Michael Jordan\", \"Prompt: Steve Curry\"]\n",
    "\n",
    "logits, v = model.run_with_cache(\n",
    "    input,\n",
    "    remove_batch_dim=False,\n",
    "    names_filter=lambda layer: \"hook_resid_post\" in layer\n",
    "    or \"blocks.0.hook_mlp_in\" in layer,  # Set to specific layers if needed\n",
    "    incl_bwd=False,\n",
    "    return_cache_object=False,\n",
    ")\n",
    "\n",
    "mj = v[\"blocks.0.hook_resid_post\"][0]\n",
    "\n",
    "logits2, v2 = model.run_with_cache(\n",
    "    input[0],\n",
    "    remove_batch_dim=True,\n",
    "#     names_filter=lambda layer: \"hook_resid_post\" in layer\n",
    "#     or \"blocks.0.hook_mlp_in\" in layer,  # Set to specific layers if needed\n",
    "    incl_bwd=False,\n",
    "    return_cache_object=False,\n",
    ")\n",
    "\n",
    "print(v['blocks.0.hook_resid_post'][0])\n",
    "for l, v in v2.items():\n",
    "    if \"blocks.0\" in l:\n",
    "        print(f\"{l}: {v.shape}\")\n",
    "\n",
    "a = v2[\"blocks.0.hook_resid_pre\"][-1]\n",
    "b = v2[\"blocks.0.hook_attn_out\"][-1]\n",
    "c = v2[\"blocks.0.hook_mlp_out\"][-1]\n",
    "print(f\"blocks.0.hook_resid_pre: {v2['blocks.0.hook_resid_pre'][-1]}\")\n",
    "print(f\"blocks.0.hook_attn_out: {v2['blocks.0.hook_attn_out'][-1]}\")\n",
    "print(f\"blocks.0.hook_mlp_out: {v2['blocks.0.hook_mlp_out'][-1]}\")\n",
    "print(f\"blocks.0.hook_resid_post: {v2['blocks.0.hook_resid_post'][-1]}\")\n",
    "print(f\"{a+b+c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c826984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-2.8b into HookedTransformer\n",
      "Processing batch of length 205 with 67 names...\n",
      "Processing batch of length 135 with 95 names...\n",
      "Processing batch of length 148 with 107 names...\n",
      "Processing batch of length 150 with 116 names...\n",
      "Processing batch of length 194 with 113 names...\n",
      "Processing batch of length 195 with 77 names...\n",
      "Processing batch of length 131 with 120 names...\n",
      "Processing batch of length 146 with 83 names...\n",
      "Processing batch of length 125 with 86 names...\n",
      "Processing batch of length 147 with 95 names...\n",
      "Processing batch of length 166 with 119 names...\n",
      "Processing batch of length 156 with 83 names...\n",
      "Processing batch of length 50 with 117 names...\n",
      "Processing batch of length 74 with 90 names...\n",
      "Processing batch of length 84 with 83 names...\n",
      "Processing batch of length 160 with 76 names...\n",
      "Processing batch of length 141 with 116 names...\n",
      "Processing batch of length 52 with 90 names...\n",
      "Processing batch of length 96 with 122 names...\n",
      "Processing batch of length 213 with 125 names...\n",
      "Processing batch of length 66 with 108 names...\n",
      "Processing batch of length 168 with 79 names...\n",
      "Processing batch of length 23 with 86 names...\n",
      "Processing batch of length 180 with 117 names...\n",
      "Processing batch of length 38 with 119 names...\n",
      "Processing batch of length 200 with 77 names...\n",
      "Processing batch of length 100 with 85 names...\n",
      "Processing batch of length 11 with 84 names...\n",
      "Processing batch of length 77 with 84 names...\n",
      "Processing batch of length 67 with 80 names...\n",
      "Processing batch of length 16 with 99 names...\n",
      "Processing batch of length 177 with 93 names...\n",
      "Processing batch of length 43 with 92 names...\n",
      "Processing batch of length 95 with 95 names...\n",
      "Processing batch of length 161 with 83 names...\n",
      "Processing batch of length 6 with 85 names...\n",
      "Processing batch of length 35 with 78 names...\n",
      "Processing batch of length 108 with 121 names...\n",
      "Processing batch of length 75 with 113 names...\n",
      "Processing batch of length 127 with 112 names...\n",
      "Processing batch of length 58 with 92 names...\n",
      "Processing batch of length 28 with 89 names...\n",
      "Processing batch of length 151 with 69 names...\n",
      "Processing batch of length 188 with 90 names...\n",
      "Processing batch of length 193 with 88 names...\n",
      "Processing batch of length 174 with 75 names...\n",
      "Processing batch of length 93 with 86 names...\n",
      "Processing batch of length 113 with 116 names...\n",
      "Processing batch of length 80 with 111 names...\n",
      "Processing batch of length 157 with 105 names...\n",
      "Processing batch of length 199 with 112 names...\n",
      "Processing batch of length 63 with 89 names...\n",
      "Processing batch of length 102 with 86 names...\n",
      "Processing batch of length 53 with 93 names...\n",
      "Processing batch of length 64 with 116 names...\n",
      "Processing batch of length 159 with 118 names...\n",
      "Processing batch of length 186 with 87 names...\n",
      "Processing batch of length 47 with 114 names...\n",
      "Processing batch of length 167 with 75 names...\n",
      "Processing batch of length 130 with 79 names...\n",
      "Processing batch of length 209 with 80 names...\n",
      "Processing batch of length 45 with 121 names...\n",
      "Processing batch of length 162 with 119 names...\n",
      "Processing batch of length 81 with 94 names...\n",
      "Processing batch of length 138 with 98 names...\n",
      "Processing batch of length 8 with 89 names...\n",
      "Processing batch of length 187 with 107 names...\n",
      "Processing batch of length 184 with 73 names...\n",
      "Processing batch of length 204 with 114 names...\n",
      "Processing batch of length 172 with 85 names...\n",
      "Processing batch of length 15 with 103 names...\n",
      "Processing batch of length 10 with 105 names...\n",
      "Processing batch of length 139 with 90 names...\n",
      "Processing batch of length 198 with 74 names...\n",
      "Processing batch of length 24 with 111 names...\n",
      "Processing batch of length 124 with 111 names...\n",
      "Processing batch of length 115 with 114 names...\n",
      "Processing batch of length 165 with 86 names...\n",
      "Processing batch of length 208 with 119 names...\n",
      "Processing batch of length 103 with 124 names...\n",
      "Processing batch of length 12 with 99 names...\n",
      "Processing batch of length 169 with 105 names...\n",
      "Processing batch of length 71 with 112 names...\n",
      "Processing batch of length 119 with 86 names...\n",
      "Processing batch of length 46 with 86 names...\n",
      "Processing batch of length 206 with 101 names...\n",
      "Processing batch of length 145 with 107 names...\n",
      "Processing batch of length 97 with 87 names...\n",
      "Processing batch of length 153 with 103 names...\n",
      "Processing batch of length 65 with 86 names...\n",
      "Processing batch of length 13 with 75 names...\n",
      "Processing batch of length 21 with 93 names...\n",
      "Processing batch of length 73 with 108 names...\n",
      "Processing batch of length 70 with 85 names...\n",
      "Processing batch of length 83 with 82 names...\n",
      "Processing batch of length 27 with 74 names...\n",
      "Processing batch of length 190 with 118 names...\n",
      "Processing batch of length 210 with 76 names...\n",
      "Processing batch of length 101 with 111 names...\n",
      "Processing batch of length 114 with 73 names...\n",
      "Processing batch of length 29 with 101 names...\n",
      "Processing batch of length 122 with 106 names...\n",
      "Processing batch of length 90 with 83 names...\n",
      "Processing batch of length 22 with 115 names...\n",
      "Processing batch of length 14 with 90 names...\n",
      "Processing batch of length 60 with 86 names...\n",
      "Processing batch of length 117 with 115 names...\n",
      "Processing batch of length 17 with 128 names...\n",
      "Processing batch of length 176 with 125 names...\n",
      "Processing batch of length 98 with 85 names...\n",
      "Processing batch of length 68 with 120 names...\n",
      "Processing batch of length 32 with 89 names...\n",
      "Processing batch of length 163 with 88 names...\n",
      "Processing batch of length 181 with 80 names...\n",
      "Processing batch of length 164 with 119 names...\n",
      "Processing batch of length 41 with 83 names...\n",
      "Processing batch of length 134 with 122 names...\n",
      "Processing batch of length 173 with 115 names...\n",
      "Processing batch of length 182 with 84 names...\n",
      "Processing batch of length 129 with 106 names...\n",
      "Processing batch of length 183 with 124 names...\n",
      "Processing batch of length 136 with 104 names...\n",
      "Processing batch of length 121 with 70 names...\n",
      "Processing batch of length 144 with 89 names...\n",
      "Processing batch of length 202 with 77 names...\n",
      "Processing batch of length 196 with 82 names...\n",
      "Processing batch of length 142 with 86 names...\n",
      "Processing batch of length 99 with 113 names...\n",
      "Processing batch of length 51 with 89 names...\n",
      "Processing batch of length 110 with 111 names...\n",
      "Processing batch of length 87 with 115 names...\n",
      "Processing batch of length 152 with 105 names...\n",
      "Processing batch of length 109 with 98 names...\n",
      "Processing batch of length 85 with 115 names...\n",
      "Processing batch of length 92 with 116 names...\n",
      "Processing batch of length 30 with 70 names...\n",
      "Processing batch of length 82 with 111 names...\n",
      "Processing batch of length 158 with 85 names...\n",
      "Processing batch of length 211 with 101 names...\n",
      "Processing batch of length 94 with 110 names...\n",
      "Processing batch of length 192 with 134 names...\n",
      "Processing batch of length 72 with 85 names...\n",
      "Processing batch of length 18 with 83 names...\n",
      "Processing batch of length 207 with 77 names...\n",
      "Processing batch of length 57 with 100 names...\n",
      "Processing batch of length 33 with 105 names...\n"
     ]
    }
   ],
   "source": [
    "# Clear memory\n",
    "if \"model\" in locals() or \"model\" in globals():\n",
    "    del model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Load model with memory optimizations\n",
    "model = transformer_lens.HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_NAME, device=\"cuda\", dtype=torch.float16\n",
    ")\n",
    "\n",
    "def sort_into_batches(names):\n",
    "    per_length = {}\n",
    "    for name in names:\n",
    "        prompt = f\"Fact: {name}\"\n",
    "        l = len(model.to_tokens(prompt)[0])\n",
    "        # print(f\"l={l} for {name}\")\n",
    "        if l in per_length:\n",
    "            per_length[l].append(name)\n",
    "        else:\n",
    "            per_length[l] = [name]\n",
    "    return per_length\n",
    "\n",
    "\n",
    "# Function to process one name with memory optimization\n",
    "def process_with_cache(names, caches):\n",
    "    batch = [f\"Fact: {name}\" for name in names]\n",
    "\n",
    "    _, cache = model.run_with_cache(\n",
    "        batch,\n",
    "        remove_batch_dim=False,\n",
    "        names_filter=lambda layer: \"hook_resid_post\" in layer\n",
    "        or \"blocks.0.hook_resid_pre\" in layer\n",
    "        or \"blocks.0.hook_attn_out\",  # Set to specific layers if needed\n",
    "        incl_bwd=False,\n",
    "        return_cache_object=False,\n",
    "    )\n",
    "    # The mid-layer activation is the sum of the pre-activation and attention output,\n",
    "    # the right hook does not exist for some reason.\n",
    "    for i, name in enumerate(names):\n",
    "        a = cache['blocks.0.hook_resid_pre'][i][-1]\n",
    "        b = cache['blocks.0.hook_attn_out'][i][-1]\n",
    "        caches[name] = {'blocks.0.hook_resid_mid': a + b}\n",
    "    for layer, v in cache.items():\n",
    "        if \"hook_resid_post\" not in layer:\n",
    "            continue\n",
    "        for i, name in enumerate(names):\n",
    "            # Move the last token's activation to CPU and detach\n",
    "            caches[name][layer] = v[i][-1].detach().cpu()\n",
    "\n",
    "    return caches\n",
    "\n",
    "def compute_caches(names):\n",
    "    batched_names = sort_into_batches(names)\n",
    "    caches = {}\n",
    "    with torch.inference_mode():\n",
    "        for length, batch in batched_names.items():\n",
    "            print(f\"Processing batch of length {length} with {len(batch)} names...\")\n",
    "            for i in range(0, len(batch), MAX_BATCH_SIZE):\n",
    "                # Process in batches of MAX_BATCH_SIZE\n",
    "                small_batch = batch[i:i + MAX_BATCH_SIZE]\n",
    "                # print(f\"{batch}\")\n",
    "                # for i, name in enumerate(names):\n",
    "                # Process each name individually and save to disk\n",
    "                try:\n",
    "                    process_with_cache(small_batch, caches)\n",
    "                    # print(name)\n",
    "                    # Save each cache separately to avoid accumulating in memory\n",
    "                    # torch.save(caches[name], f\"cache/cache_{i}_{name.replace(' ', '_')}.pt\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing batch for length {length}: {e}\")\n",
    "\n",
    "                # Force cleanup\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                # if i % 1000 == 0:\n",
    "                #     print(f\"Step {i} of {len(names)}...\")\n",
    "                # print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "                # print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "    return caches\n",
    "\n",
    "# Load player names\n",
    "with open(f\"{DATA_FOLDER}/basketball_players.txt\", \"r\") as f:\n",
    "    player_names = [line.rstrip() for line in f if line.rstrip()]\n",
    "with open(f\"{DATA_FOLDER}/fake_basketball_players.txt\", \"r\") as f:\n",
    "    fake_names = [line.rstrip() for line in f if line.rstrip()]\n",
    "\n",
    "player_caches = compute_caches(player_names[:LIMIT])\n",
    "print(f\"Processed {len(player_caches)} player caches.\")\n",
    "fake_caches = compute_caches(fake_names[:LIMIT])\n",
    "print(f\"Processed {len(fake_caches)} fake player caches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4032f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in player_caches:\n",
    "    for key in player_caches[name]:\n",
    "        player_caches[name][key] = player_caches[name][key].detach().cpu()\n",
    "        print(f\"{name.strip()}.{key}: {player_caches[name][key].shape} {player_caches[name][key]}\")\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_name(layer):\n",
    "    layer_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    if layer == -1:\n",
    "        layer_name = \"blocks.0.hook_resid_mid\"\n",
    "    return layer_name\n",
    "\n",
    "\n",
    "def make_point_thick(vector, extra_vectors=100, thickness=0.2):\n",
    "    # Create the points +- thickness in each dimension, return vector of all points and original\n",
    "    vectors = [vector]\n",
    "    for dim in range(min(extra_vectors, len(vector))):\n",
    "        # Create a new vector with the current dimension increased and decreased by thickness\n",
    "        pos_vector = vector.copy()\n",
    "        pos_vector[dim] += thickness\n",
    "        vectors.append(pos_vector)\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def get_point_set(\n",
    "    caches, hook_name=\"blocks.0.hook_resid_post\", extra_vectors=0, thickness=0.2\n",
    "):\n",
    "    # Create a list to hold all vectors\n",
    "    vectors = []\n",
    "\n",
    "    # Extract vectors from all players\n",
    "    print(f\"Collecting vectors from layer {hook_name}...\")\n",
    "    for name in caches.keys():\n",
    "        try:\n",
    "            # Extract the vector for this player\n",
    "            vector = caches[name][hook_name]\n",
    "\n",
    "            # Make sure it's on CPU and detached (in case it isn't already)\n",
    "            vector = vector.detach().cpu()\n",
    "            # Convert the tensor to a numpy array\n",
    "            vector = vector.numpy()\n",
    "            # Add to our collection\n",
    "            vectors.extend(\n",
    "                make_point_thick(\n",
    "                    vector, extra_vectors=extra_vectors, thickness=thickness\n",
    "                )\n",
    "            )\n",
    "        except KeyError as e:\n",
    "            print(f\"Skipping {name}: Missing '{hook_name}' in cache\")\n",
    "        except IndexError as e:\n",
    "            print(f\"Skipping {name}: No last element in '{hook_name}'\")\n",
    "    return vectors\n",
    "\n",
    "\n",
    "player_layer_0 = get_point_set(\n",
    "    player_caches, \"blocks.0.hook_resid_mid\", extra_vectors=0\n",
    ")\n",
    "fake_layer_0 = get_point_set(\n",
    "    fake_caches, \"blocks.0.hook_resid_mid\", extra_vectors=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73683f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_linear_separation(set_a, set_b):\n",
    "    # Combine all vectors into a dataset and create labels\n",
    "    X = np.vstack([set_a, set_b])\n",
    "    y = np.array([\"Player\"] * len(set_a) + [\"Fake\"] * len(set_b))\n",
    "\n",
    "    # Train a linear SVM classifier\n",
    "    clf = LinearSVC(random_state=42, max_iter=10000)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X)\n",
    "\n",
    "    # Count overall misclassifications\n",
    "    misclassified = np.sum(y_pred != y)\n",
    "    print(\n",
    "        f\"Total misclassified points: {misclassified} out of {len(y)} ({misclassified/len(y)*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Get misclassified by class\n",
    "    fake_misclassified = np.sum((y_pred != y) & (y_pred == \"Player\") & (y == \"Fake\"))\n",
    "    player_misclassified = np.sum((y_pred != y) & (y_pred == \"Fake\") & (y == \"Player\"))\n",
    "\n",
    "    print(f\"Fake points misclassified as players: {fake_misclassified}\")\n",
    "    print(f\"Player points misclassified as fake: {player_misclassified}\")\n",
    "    return fake_misclassified, player_misclassified\n",
    "\n",
    "\n",
    "print(count_linear_separation(player_layer_0, fake_layer_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot neighborhood visualization, compute linear separation\n",
    "# Initialize lists to store results\n",
    "layer_results = []\n",
    "misclassification_rates = []\n",
    "player_misclassified_counts = []\n",
    "fake_misclassified_counts = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Get the number of layers\n",
    "num_layers = model.W_K.shape[0]\n",
    "print(f\"Analyzing linear separability across {num_layers} layers...\")\n",
    "\n",
    "plot_every = 4\n",
    "\n",
    "n_rows = (num_layers // plot_every + 2) // 3 + 1\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=n_rows,\n",
    "    ncols=3,\n",
    "    figsize=(12, 4 * n_rows),\n",
    ")\n",
    "ax_idx = 0\n",
    "\n",
    "# Process each layer\n",
    "for layer in range(-1, num_layers):\n",
    "    # Get vectors from this layer using existing functions\n",
    "    layer_name = get_layer_name(layer)\n",
    "    player_layer_vectors = get_point_set(\n",
    "        player_caches, layer_name, extra_vectors=0\n",
    "    )\n",
    "    fake_layer_vectors = get_point_set(fake_caches, layer_name, extra_vectors=0)\n",
    "\n",
    "    # Check linear separability using existing function\n",
    "    print(f\"\\nLayer {layer}:\")\n",
    "    fake_mis, player_mis = count_linear_separation(player_layer_vectors, fake_layer_vectors)\n",
    "\n",
    "    # Calculate metrics\n",
    "    total_points = len(player_layer_vectors) + len(fake_layer_vectors)\n",
    "    total_mis = fake_mis + player_mis\n",
    "    accuracy = 1 - (total_mis / total_points)\n",
    "\n",
    "    # Store results\n",
    "    layer_results.append({\n",
    "        \"layer\": layer,\n",
    "        \"player_count\": len(player_layer_vectors),\n",
    "        \"fake_count\": len(fake_layer_vectors),\n",
    "        \"fake_misclassified\": fake_mis,\n",
    "        \"player_misclassified\": player_mis,\n",
    "        \"total_misclassified\": total_mis,\n",
    "        \"accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "    misclassification_rates.append(total_mis / total_points)\n",
    "    player_misclassified_counts.append(player_mis)\n",
    "    fake_misclassified_counts.append(fake_mis)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "    # Generate TSNE visualization for selected layers (to avoid too many plots)\n",
    "    if layer % plot_every == 0 or layer == -1 or layer == num_layers - 1:\n",
    "        # Combine vectors for visualization\n",
    "        all_vectors = np.vstack([player_layer_vectors, fake_layer_vectors])\n",
    "        # labels = [\"player\"] * len(player_layer_vectors) + [\"fake\"] * len(fake_layer_vectors)\n",
    "        # def describe_prefix(name):\n",
    "        #     spaces = len(name) - len(name.lstrip())\n",
    "        #     if spaces == 0:\n",
    "        #         return \"no prefix\"\n",
    "        #     return f\"<{(spaces//500 + 1) * 500} spaces\"\n",
    "        def describe_prefix(name):\n",
    "            first_letter = name.lstrip()[0]\n",
    "            return f\"starts with {first_letter}\"\n",
    "        labels = [describe_prefix(name) for name in player_caches.keys()] + [describe_prefix(name) for name in fake_caches.keys()]\n",
    "        # Apply TSNE\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_vectors)//5))\n",
    "        embeddings = tsne.fit_transform(all_vectors)\n",
    "\n",
    "        # Create plot\n",
    "        scatter = sns.scatterplot(x=embeddings[:, 0], y=embeddings[:, 1], \n",
    "                hue=labels, palette=\"Set1\", ax = axs.flat[ax_idx])\n",
    "        if layer != 20:\n",
    "            scatter.legend_.remove()\n",
    "        axs.flat[ax_idx].set_title(f\"Layer {layer}\")\n",
    "        ax_idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot misclassification rates across layers\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(-1, num_layers), misclassification_rates, marker='o', color='red')\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Misclassification Rate')\n",
    "plt.title('Linear Classifier Misclassification Rate by Layer')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy across layers\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(-1, num_layers), accuracy_scores, marker='o', color='green')\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Classification Accuracy')\n",
    "plt.title('Linear Classifier Accuracy by Layer')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Plot misclassifications by category\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(-1, num_layers)\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, player_misclassified_counts, width, label='Players Misclassified as Fake')\n",
    "plt.bar(x + width/2, fake_misclassified_counts, width, label='Fake Misclassified as Players')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Number of Misclassifications')\n",
    "plt.title('Misclassification Counts by Category')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find best layer for linear separation\n",
    "best_layer = np.argmax(accuracy_scores)\n",
    "print(f\"\\nBest layer for linear separation: Layer {best_layer}\")\n",
    "print(f\"Accuracy: {accuracy_scores[best_layer]*100:.2f}%\")\n",
    "print(f\"Misclassification rate: {misclassification_rates[best_layer]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distance_stats(set_a, set_b, stats_lists=None, equal=False):\n",
    "    \"\"\"\n",
    "    Calculate and print distance statistics between two sets of vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - set_a: First set of vectors\n",
    "    - set_b: Second set of vectors\n",
    "    - stats_lists: Optional dictionary containing lists to append stats to\n",
    "                  with keys 'min', 'max', 'mean', 'median'\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for i, a in enumerate(set_a):\n",
    "        for j, b in enumerate(set_b):\n",
    "            distance = np.linalg.norm(a - b)\n",
    "            if equal and i == j:\n",
    "                # Skip distances between the same points\n",
    "                continue\n",
    "            distances.append(distance)\n",
    "    distances.sort()\n",
    "    print(\n",
    "        f\"Points in A: {len(set_a)}, in B: {len(set_b)}, total distances: {len(distances)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Min: {distances[0]:.4f}, Mean: {np.mean(distances):.4f}, Median: {np.median(distances):.4f}, Max: {distances[-1]:.4f}\"\n",
    "    )\n",
    "\n",
    "    if stats_lists is not None:\n",
    "        if \"min\" in stats_lists:\n",
    "            stats_lists[\"min\"].append(distances[0])\n",
    "        if \"max\" in stats_lists:\n",
    "            stats_lists[\"max\"].append(distances[-1])\n",
    "        if \"mean\" in stats_lists:\n",
    "            stats_lists[\"mean\"].append(np.mean(distances))\n",
    "        if \"median\" in stats_lists:\n",
    "            stats_lists[\"median\"].append(np.median(distances))\n",
    "\n",
    "\n",
    "print_distance_stats(player_layer_0[:1000], fake_layer_0[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def get_min_distance(points_a, points_b):\n",
    "    distances = cdist(points_a, points_b, metric=\"euclidean\")\n",
    "    min_dist = np.min(distances)\n",
    "    i, j = np.unravel_index(np.argmin(distances), distances.shape)\n",
    "    return min_dist, i, j\n",
    "\n",
    "# print(min_distance(player_layer_0, fake_layer_0))\n",
    "# 16s: (np.float64(1.7858378876167593), np.int64(1760), np.int64(137))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda7eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from convex_point_cover.algorithms.fast_kruskal import fast_kruskal\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "num_layers = model.W_K.shape[0]\n",
    "layers = range(-1, num_layers)\n",
    "\n",
    "\n",
    "def process_layer(layer, player_vectors, fake_vectors):\n",
    "    print(f\"Processing layer {layer}...\")\n",
    "    min_distance, _, _ = get_min_distance(player_vectors, fake_vectors)\n",
    "    print(f\"Layer {layer}: min distance={min_distance}\")\n",
    "    \n",
    "    delta = min_distance - 0.11\n",
    "\n",
    "    positive_clusters = fast_kruskal(\n",
    "        player_vectors,\n",
    "        fake_vectors,\n",
    "        epsilon=0.1,\n",
    "        delta=delta,\n",
    "        debug=False,\n",
    "    )\n",
    "    negative_clusters = fast_kruskal(\n",
    "        fake_vectors,\n",
    "        player_vectors,\n",
    "        epsilon=0.1,\n",
    "        delta=delta,\n",
    "        debug=False,\n",
    "    )\n",
    "    return {\n",
    "        \"layer\": layer,\n",
    "        \"player_count\": len(player_vectors),\n",
    "        \"fake_count\": len(fake_vectors),\n",
    "        \"positive_clusters\": len(positive_clusters),\n",
    "        \"negative_clusters\": len(negative_clusters),\n",
    "    }\n",
    "\n",
    "# Run parallel computation - will use all available cores\n",
    "print(\"Processing layers in parallel...\")\n",
    "results = Parallel(n_jobs=NUM_JOBS, verbose=10)(\n",
    "    delayed(process_layer)(\n",
    "        layer,\n",
    "        get_point_set(\n",
    "            player_caches,\n",
    "            get_layer_name(layer),\n",
    "            extra_vectors=0,\n",
    "            thickness=0.2,\n",
    "        ),\n",
    "        get_point_set(\n",
    "            fake_caches,\n",
    "            get_layer_name(layer),\n",
    "            extra_vectors=0,\n",
    "            thickness=0.2,\n",
    "        ),\n",
    "    )\n",
    "    for layer in layers\n",
    ")\n",
    "\n",
    "# Process and display results\n",
    "cluster_sizes = [[], []]\n",
    "for result in sorted(results, key=lambda x: x[\"layer\"]):\n",
    "    layer = result[\"layer\"]\n",
    "    player_count = result[\"player_count\"]\n",
    "    fake_count = result[\"fake_count\"]\n",
    "    pos_clusters = result[\"positive_clusters\"]\n",
    "    neg_clusters = result[\"negative_clusters\"]\n",
    "\n",
    "    # Store results\n",
    "    cluster_sizes[0].append(pos_clusters)\n",
    "    cluster_sizes[1].append(neg_clusters)\n",
    "\n",
    "    # Print layer info\n",
    "    print(\n",
    "        f\"Layer {layer}: {player_count} player vectors collected, {fake_count} fake vectors collected.\"\n",
    "    )\n",
    "    print(f\"{pos_clusters} clusters found.\")\n",
    "    print(f\"{neg_clusters} negative clusters found.\")\n",
    "\n",
    "# Plot the number of clusters found across layers\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(layers, cluster_sizes[0], marker=\"o\", label=\"Positive Clusters\")\n",
    "plt.plot(layers, cluster_sizes[1], marker=\"s\", label=\"Negative Clusters\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Number of Clusters\")\n",
    "plt.title(\"Number of Clusters Found Across Layers\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
